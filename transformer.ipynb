{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9429704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/kkiko/Desktop/CAA/aa/CAA_P1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-01 00:58:06.885444: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-01 00:58:08.806409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746057489.434760   71215 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746057489.619626   71215 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746057491.197667   71215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746057491.197689   71215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746057491.197691   71215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746057491.197693   71215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-01 00:58:11.358512: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFViTForImageClassification, ViTFeatureExtractor\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3949e453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c2f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_vit_pretrained(input_size: tuple[int, int], num_classes: int, fine_tune: bool = False, model_name: str = 'google/vit-small-patch16-224'):\n",
    "    \"\"\"\n",
    "    Creates an image classification model using a pre-trained Vision Transformer (ViT).\n",
    "\n",
    "    Args:\n",
    "        input_size (tuple[int, int]): Target input image size (height, width) expected by the ViT model.\n",
    "                                      The feature extractor will handle resizing images to this size.\n",
    "        num_classes (int): Number of output classes for the classification head.\n",
    "        fine_tune (bool): If True, unfreezes the weights of the pre-trained\n",
    "                          ViT base model for fine-tuning. Defaults to False.\n",
    "        model_name (str): The name of the pre-trained ViT model to load from\n",
    "                          Hugging Face Hub. Defaults to 'google/vit-small-patch16-224'.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A Keras model ready for compilation and training.\n",
    "        ViTFeatureExtractor: The feature extractor for preprocessing images.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained ViT model for image classification.\n",
    "    # We specify the number of labels and explicitly ignore the mismatch\n",
    "    # in the classification head dimensions, as we intend to train it.\n",
    "    print(f\"Loading pre-trained model: {model_name}\")\n",
    "    model = TFViTForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True # Allows loading a new classification head\n",
    "    )\n",
    "\n",
    "    # Load the feature extractor associated with the model.\n",
    "    # This handles resizing, normalization, etc.\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "    # --- Freezing/Unfreezing Layers ---\n",
    "    base_model = model.vit # Access the base ViT model\n",
    "\n",
    "    if not fine_tune:\n",
    "        print(\"Freezing base ViT model weights.\")\n",
    "        base_model.trainable = False\n",
    "    else:\n",
    "        print(\"Unfreezing base ViT model weights for fine-tuning.\")\n",
    "        base_model.trainable = True\n",
    "\n",
    "    # The classification head added by TFViTForImageClassification is automatically trainable.\n",
    "\n",
    "    print(f\"Model '{model_name}' loaded.\")\n",
    "    # Use the size defined by the feature extractor\n",
    "    print(f\"Input size expected by feature extractor: {feature_extractor.size}\")\n",
    "    print(f\"Number of output classes: {num_classes}\")\n",
    "    print(f\"Base model trainable: {base_model.trainable}\")\n",
    "\n",
    "    # Note: The model is returned uncompiled. Compilation should happen before training.\n",
    "    return model, feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ece8669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ViT Model: google/vit-base-patch16-224\n",
      "Expected Input Size: (512, 384)\n"
     ]
    }
   ],
   "source": [
    "VIT_MODEL_NAME = 'google/vit-base-patch16-224' # Smaller ViT model\n",
    "NUM_CLASSES = 6 # cardboard, glass, metal, paper, plastic, trash\n",
    "BATCH_SIZE = 32 # Adjust based on GPU memory\n",
    "EPOCHS = 100 # Number of training epochs (adjust as needed)\n",
    "LEARNING_RATE = 3e-5 # Common learning rate for ViT fine-tuning/transfer\n",
    "FINE_TUNE_BASE = False # Start with frozen base model (transfer learning)\n",
    "\n",
    "IMG_HEIGHT = 512\n",
    "IMG_WIDTH = 384\n",
    "INPUT_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
    "\n",
    "print(f\"Using ViT Model: {VIT_MODEL_NAME}\")\n",
    "print(f\"Expected Input Size: {INPUT_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59138e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model: google/vit-base-patch16-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746057522.098535   71215 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13499 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2025-05-01 00:58:43.044769: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Input to reshape is a tensor with 768000 values, but the requested shape has 4608\n",
      "2025-05-01 00:58:43.045426: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Input to reshape is a tensor with 1000 values, but the requested shape has 6\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFViTForImageClassification: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFViTForImageClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n",
      "Some weights of TFViTForImageClassification were not initialized from the model checkpoint are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape (1000, 768) in the checkpoint and (768, 6) in the model instantiated\n",
      "- classifier.bias: found shape (1000,) in the checkpoint and (6,) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing base ViT model weights.\n",
      "Model 'google/vit-base-patch16-224' loaded.\n",
      "Input size expected by feature extractor: {'height': 224, 'width': 224}\n",
      "Number of output classes: 6\n",
      "Base model trainable: False\n",
      "Model 'vit1' set with 85803270 parameters.\n",
      "Feature extractor requires input size: {'height': 224, 'width': 224}\n",
      "Model: \"tf_vi_t_for_image_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vit (TFViTMainLayer)        multiple                  85798656  \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  4614      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85803270 (327.31 MB)\n",
      "Trainable params: 4614 (18.02 KB)\n",
      "Non-trainable params: 85798656 (327.30 MB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/kkiko/Desktop/CAA/aa/CAA_P1/.venv/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_vit , feature_extractor = model_vit_pretrained(INPUT_SIZE, NUM_CLASSES, fine_tune=FINE_TUNE_BASE, model_name=VIT_MODEL_NAME)\n",
    "\n",
    "model_vit1 = LModel(\"vit1\")\n",
    "model_vit1.set_model(model_vit, feature_extractor)\n",
    "model_vit1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8821141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n",
      "Model compiled.\n"
     ]
    }
   ],
   "source": [
    "model_vit1.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f8a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2274 images belonging to 6 classes.\n",
      "Found 253 images belonging to 6 classes.\n",
      "Class Indices: {'cardboard': 0, 'glass': 1, 'metal': 2, 'paper': 3, 'plastic': 4, 'trash': 5}\n",
      "No cache found or cache=False. Training from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training via _fit using Sequence for 100 epochs...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746057530.020733   71495 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746057534.302366   71493 service.cc:152] XLA service 0x7f8c7e16fc70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1746057534.302393   71493 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4080 SUPER, Compute Capability 8.9\n",
      "2025-05-01 00:58:54.418576: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/72 [..............................] - ETA: 6s - loss: 7.4387 - accuracy: 0.1094       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746057534.729811   71493 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 48s 533ms/step - loss: 9.0987 - accuracy: 0.1931 - val_loss: 9.3014 - val_accuracy: 0.2055\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 37s 523ms/step - loss: 9.1152 - accuracy: 0.1974 - val_loss: 9.3014 - val_accuracy: 0.2055\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 37s 514ms/step - loss: 9.1152 - accuracy: 0.1974 - val_loss: 9.3014 - val_accuracy: 0.2055\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 36s 505ms/step - loss: 9.1152 - accuracy: 0.1974 - val_loss: 9.3014 - val_accuracy: 0.2055\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 36s 509ms/step - loss: 9.1152 - accuracy: 0.1974 - val_loss: 9.3014 - val_accuracy: 0.2055\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 36s 500ms/step - loss: 9.1152 - accuracy: 0.1974 - val_loss: 9.3014 - val_accuracy: 0.2055\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 36s 502ms/step - loss: 9.1152 - accuracy: 0.1974 - val_loss: 9.3014 - val_accuracy: 0.2055\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 35s 498ms/step - loss: 9.1152 - accuracy: 0.1974 - val_loss: 9.3014 - val_accuracy: 0.2055\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 36s 503ms/step - loss: 9.1152 - accuracy: 0.1974 - val_loss: 9.3014 - val_accuracy: 0.2055\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 36s 503ms/step - loss: 9.1152 - accuracy: 0.1974 - val_loss: 9.3014 - val_accuracy: 0.2055\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 36s 507ms/step - loss: 9.1152 - accuracy: 0.1974 - val_loss: 9.3014 - val_accuracy: 0.2055\n",
      "Epoch 12/100\n",
      "58/72 [=======================>......] - ETA: 10s - loss: 9.1880 - accuracy: 0.1945"
     ]
    }
   ],
   "source": [
    "hist_vit1 = model_vit1.fit(\n",
    "    train_path=\"train\",\n",
    "    test_path=\"test\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    model_save_dir=\"models/vit1\",\n",
    "    cache=True,\n",
    "    continue_training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eca7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hist_vit1: # Only evaluate if training was successful or loaded\n",
    "    print(\"\\n--- Starting Evaluation Phase ---\")\n",
    "    model_vit1.evaluate(model_save_dir=\"models/vit1\")\n",
    "else:\n",
    "    print(\"Skipping evaluation as training did not produce a history.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
