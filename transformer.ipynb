{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9429704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/kkiko/Desktop/CAA/aa/CAA_P1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-01 02:00:57.986236: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-01 02:01:00.016475: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746061260.768337   86068 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746061260.961954   86068 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746061262.684940   86068 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746061262.684959   86068 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746061262.684960   86068 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746061262.684961   86068 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-01 02:01:02.849656: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFViTForImageClassification, ViTFeatureExtractor\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3949e453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c2f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_vit_pretrained(input_size: tuple[int, int], num_classes: int, fine_tune: bool = False, model_name: str = 'google/vit-small-patch16-224'):\n",
    "    \"\"\"\n",
    "    Creates an image classification model using a pre-trained Vision Transformer (ViT).\n",
    "\n",
    "    Args:\n",
    "        input_size (tuple[int, int]): Target input image size (height, width) expected by the ViT model.\n",
    "                                      The feature extractor will handle resizing images to this size.\n",
    "        num_classes (int): Number of output classes for the classification head.\n",
    "        fine_tune (bool): If True, unfreezes the weights of the pre-trained\n",
    "                          ViT base model for fine-tuning. Defaults to False.\n",
    "        model_name (str): The name of the pre-trained ViT model to load from\n",
    "                          Hugging Face Hub. Defaults to 'google/vit-small-patch16-224'.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A Keras model ready for compilation and training.\n",
    "        ViTFeatureExtractor: The feature extractor for preprocessing images.\n",
    "    \"\"\"\n",
    "    # Load the pre-trained ViT model for image classification.\n",
    "    # We specify the number of labels and explicitly ignore the mismatch\n",
    "    # in the classification head dimensions, as we intend to train it.\n",
    "    print(f\"Loading pre-trained model: {model_name}\")\n",
    "    model = TFViTForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True # Allows loading a new classification head\n",
    "    )\n",
    "\n",
    "    # Load the feature extractor associated with the model.\n",
    "    # This handles resizing, normalization, etc.\n",
    "    feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "    # --- Freezing/Unfreezing Layers ---\n",
    "    base_model = model.vit # Access the base ViT model\n",
    "\n",
    "    if not fine_tune:\n",
    "        print(\"Freezing base ViT model weights.\")\n",
    "        base_model.trainable = False\n",
    "    else:\n",
    "        print(\"Unfreezing base ViT model weights for fine-tuning.\")\n",
    "        base_model.trainable = True\n",
    "\n",
    "    # The classification head added by TFViTForImageClassification is automatically trainable.\n",
    "\n",
    "    print(f\"Model '{model_name}' loaded.\")\n",
    "    # Use the size defined by the feature extractor\n",
    "    print(f\"Input size expected by feature extractor: {feature_extractor.size}\")\n",
    "    print(f\"Number of output classes: {num_classes}\")\n",
    "    print(f\"Base model trainable: {base_model.trainable}\")\n",
    "\n",
    "    # Note: The model is returned uncompiled. Compilation should happen before training.\n",
    "    return model, feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ece8669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ViT Model: google/vit-base-patch16-224\n",
      "Expected Input Size: (512, 384)\n"
     ]
    }
   ],
   "source": [
    "VIT_MODEL_NAME = 'google/vit-base-patch16-224' # Smaller ViT model\n",
    "NUM_CLASSES = 6 # cardboard, glass, metal, paper, plastic, trash\n",
    "BATCH_SIZE = 32 # Adjust based on GPU memory\n",
    "EPOCHS = 100 # Number of training epochs (adjust as needed)\n",
    "LEARNING_RATE = 3e-5 # Common learning rate for ViT fine-tuning/transfer\n",
    "FINE_TUNE_BASE = False # Start with frozen base model (transfer learning)\n",
    "\n",
    "IMG_HEIGHT = 512\n",
    "IMG_WIDTH = 384\n",
    "INPUT_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
    "\n",
    "print(f\"Using ViT Model: {VIT_MODEL_NAME}\")\n",
    "print(f\"Expected Input Size: {INPUT_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59138e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model: google/vit-base-patch16-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746061294.516719   86068 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13499 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080 SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2025-05-01 02:01:36.641708: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Input to reshape is a tensor with 768000 values, but the requested shape has 4608\n",
      "2025-05-01 02:01:36.642305: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Input to reshape is a tensor with 1000 values, but the requested shape has 6\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFViTForImageClassification: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFViTForImageClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n",
      "Some weights of TFViTForImageClassification were not initialized from the model checkpoint are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape (1000, 768) in the checkpoint and (768, 6) in the model instantiated\n",
      "- classifier.bias: found shape (1000,) in the checkpoint and (6,) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing base ViT model weights.\n",
      "Model 'google/vit-base-patch16-224' loaded.\n",
      "Input size expected by feature extractor: {'height': 224, 'width': 224}\n",
      "Number of output classes: 6\n",
      "Base model trainable: False\n",
      "Model 'vit1' set with 85803270 parameters.\n",
      "Feature extractor requires input size: {'height': 224, 'width': 224}\n",
      "Model: \"tf_vi_t_for_image_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vit (TFViTMainLayer)        multiple                  85798656  \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  4614      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85803270 (327.31 MB)\n",
      "Trainable params: 4614 (18.02 KB)\n",
      "Non-trainable params: 85798656 (327.30 MB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/kkiko/Desktop/CAA/aa/CAA_P1/.venv/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_vit , feature_extractor = model_vit_pretrained(INPUT_SIZE, NUM_CLASSES, fine_tune=FINE_TUNE_BASE, model_name=VIT_MODEL_NAME)\n",
    "\n",
    "model_vit1 = LModel(\"vit1\")\n",
    "model_vit1.set_model(model_vit, feature_extractor)\n",
    "model_vit1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8821141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n",
      "Model compiled.\n"
     ]
    }
   ],
   "source": [
    "model_vit1.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "452f8a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2274 images belonging to 6 classes.\n",
      "Found 253 images belonging to 6 classes.\n",
      "Class Indices: {'cardboard': 0, 'glass': 1, 'metal': 2, 'paper': 3, 'plastic': 4, 'trash': 5}\n",
      "Using cached model and history without further training.\n"
     ]
    }
   ],
   "source": [
    "hist_vit1 = model_vit1.fit(\n",
    "    train_path=\"train\",\n",
    "    test_path=\"test\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    model_save_dir=\"models/vit1\",\n",
    "    cache=True,\n",
    "    continue_training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92eca7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping evaluation as training did not produce a history.\n"
     ]
    }
   ],
   "source": [
    "if hist_vit1: # Only evaluate if training was successful or loaded\n",
    "    print(\"\\n--- Starting Evaluation Phase ---\")\n",
    "    model_vit1.evaluate(model_save_dir=\"models/vit1\")\n",
    "else:\n",
    "    print(\"Skipping evaluation as training did not produce a history.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
